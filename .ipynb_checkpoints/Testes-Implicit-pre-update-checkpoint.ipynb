{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fba394c2",
   "metadata": {},
   "source": [
    "### IMPORT DE BIBLIOTECAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e8b6da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import implicit\n",
    "\n",
    "from scipy.sparse import csr_matrix, save_npz, load_npz\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# from surprise import Dataset, Reader, SVD, SVDpp, KNNWithMeans\n",
    "# from surprise.model_selection import train_test_split, cross_validate\n",
    "# from surprise import accuracy\n",
    "# from surprise.model_selection import cross_validate\n",
    "\n",
    "# from sklearn.neighbors import NearestNeighbors\n",
    "# from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances\n",
    "import pickle\n",
    "# import flask\n",
    "from tqdm import tqdm\n",
    "from deep_translator import GoogleTranslator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ce625f",
   "metadata": {},
   "source": [
    "### Carregamento de dados de categorias e histórico de clicks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ab95f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_de_cat = pd.read_csv(os.getcwd()+'\\.txt\\category_de.txt') # DataFrame Informativo das categorias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c880ae7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 696606 entries, 0 to 696605\n",
      "Data columns (total 9 columns):\n",
      " #   Column       Non-Null Count   Dtype \n",
      "---  ------       --------------   ----- \n",
      " 0   UserId       696606 non-null  object\n",
      " 1   OfferId      696606 non-null  object\n",
      " 2   OfferViewId  696606 non-null  object\n",
      " 3   CountryCode  696606 non-null  object\n",
      " 4   Category     696606 non-null  int64 \n",
      " 5   Source       696606 non-null  object\n",
      " 6   UtcDate      696606 non-null  object\n",
      " 7   Keywords     2886 non-null    object\n",
      " 8   OfferTitle   693966 non-null  object\n",
      "dtypes: int64(1), object(8)\n",
      "memory usage: 47.8+ MB\n"
     ]
    }
   ],
   "source": [
    "clicks_de = pd.read_csv(os.getcwd()+'\\.txt\\clicks_de_sample_2.txt', sep = ',', header=0) # df de histórico de clicks\n",
    "clicks_de.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27eaec20",
   "metadata": {},
   "source": [
    "Valores missing na coluna de OfferTitle que serão excluídos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48e3b58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_de = clicks_de[clicks_de.OfferTitle.isna() == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88852578",
   "metadata": {},
   "source": [
    "### CRIAÇÃO DE COLUNA DE CLICKS POR CATEGORIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44cc3cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_de.drop(columns = ['Keywords'], axis = 1, inplace=True)  # DROP DAS KEYWORDS\n",
    "\n",
    "df_de_cat.rename({'Ancertor_ID':'Ancestor_ID'}, axis = 1, inplace = True) # RENAME DO ANCESTOR_ID\n",
    "\n",
    "df_de_cat.drop('Unnamed: 0', axis =1, inplace = True) # Remoção de coluna Unnamed\n",
    "\n",
    "clicks_de['Cat_clicks'] = clicks_de.groupby('Category')['OfferId'].transform('count') # Criação de coluna de clicks por categoria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b1b118",
   "metadata": {},
   "source": [
    "### Conversão das colunas de usuário e oferta em categórica e criando novas colunas com os códigos adotados "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33f19920",
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_de.UserId = clicks_de.UserId.astype('category')\n",
    "clicks_de.OfferId = clicks_de.OfferId.astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f48eb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_de['User'] = clicks_de.UserId.cat.codes\n",
    "clicks_de['Offer'] = clicks_de.OfferId.cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f712ece2",
   "metadata": {},
   "source": [
    "### Merge do Dataframe de categorias com o dataframe de clicks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5afef22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_de = clicks_de.merge(df_de_cat, left_on = 'Category', right_on = 'ID').drop(['ID'], axis = 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849f3de7",
   "metadata": {},
   "source": [
    "### Criação de coluna com o nº total de clicks do usuário e filtragem do dataframe apenas com usuários que clicaram 10 ou mais vezes em algum produto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13b7a971",
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_de['UserTotalClicks'] = clicks_de.groupby(by=['User'])['OfferId'].transform('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8424c74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cap minimo de clicks para integrar o sistema de recomendação\n",
    "clicks_de_filtered = clicks_de[(clicks_de.UserTotalClicks > 10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef6a27c",
   "metadata": {},
   "source": [
    "\n",
    "### Agrupamento de dos clicks de usuário em ofertas únicas para termos a quantidade de cada usuário em cada oferta.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10f85895",
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_per_user_product = clicks_de_filtered.groupby(by=['User','Offer']).count()['UserTotalClicks'].reset_index().rename({'UserTotalClicks':'UserClicks'}, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4cbbb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_de['ProductClicks'] = clicks_de.groupby(by='Offer')['OfferId'].transform('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61af7fb3",
   "metadata": {},
   "source": [
    "### Criação de matrizes esparsas Usuário-item e item-usuário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb88301e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\gabri\\\\Desktop\\\\DH\\\\PI'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ab3fd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 40\n",
    "sparse_item_user = csr_matrix((clicks_per_user_product['UserClicks'], (clicks_per_user_product['Offer'], clicks_per_user_product['User'])))\n",
    "#sparse_user_item = csr_matrix((clicks_per_user_product['UserClicks'].astype(float), (clicks_per_user_product['User'], clicks_per_user_product['Offer'])))\n",
    "sparse_item_user = (sparse_item_user*alpha).astype('double') # Conversão de tipo para que o modelo ALS funcione corretamente\n",
    "sparse_user_item = sparse_item_user.T.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5c96555",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/.npz/sparse_user_item.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43msave_npz\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/.npz/sparse_user_item.npz\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse_user_item\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m save_npz(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/.npz/sparse_item_user.npz\u001b[39m\u001b[38;5;124m\"\u001b[39m, sparse_item_user)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\implicit_env\\lib\\site-packages\\scipy\\sparse\\_matrix_io.py:70\u001b[0m, in \u001b[0;36msave_npz\u001b[1;34m(file, matrix, compressed)\u001b[0m\n\u001b[0;32m     64\u001b[0m arrays_dict\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39mmatrix\u001b[38;5;241m.\u001b[39mformat\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mascii\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     66\u001b[0m     shape\u001b[38;5;241m=\u001b[39mmatrix\u001b[38;5;241m.\u001b[39mshape,\n\u001b[0;32m     67\u001b[0m     data\u001b[38;5;241m=\u001b[39mmatrix\u001b[38;5;241m.\u001b[39mdata\n\u001b[0;32m     68\u001b[0m )\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compressed:\n\u001b[1;32m---> 70\u001b[0m     np\u001b[38;5;241m.\u001b[39msavez_compressed(file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39marrays_dict)\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     np\u001b[38;5;241m.\u001b[39msavez(file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39marrays_dict)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36msavez_compressed\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\implicit_env\\lib\\site-packages\\numpy\\lib\\npyio.py:683\u001b[0m, in \u001b[0;36msavez_compressed\u001b[1;34m(file, *args, **kwds)\u001b[0m\n\u001b[0;32m    620\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_savez_compressed_dispatcher)\n\u001b[0;32m    621\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msavez_compressed\u001b[39m(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m    622\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    623\u001b[0m \u001b[38;5;124;03m    Save several arrays into a single file in compressed ``.npz`` format.\u001b[39;00m\n\u001b[0;32m    624\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    681\u001b[0m \n\u001b[0;32m    682\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 683\u001b[0m     \u001b[43m_savez\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\implicit_env\\lib\\site-packages\\numpy\\lib\\npyio.py:709\u001b[0m, in \u001b[0;36m_savez\u001b[1;34m(file, args, kwds, compress, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    707\u001b[0m     compression \u001b[38;5;241m=\u001b[39m zipfile\u001b[38;5;241m.\u001b[39mZIP_STORED\n\u001b[1;32m--> 709\u001b[0m zipf \u001b[38;5;241m=\u001b[39m \u001b[43mzipfile_factory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    711\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, val \u001b[38;5;129;01min\u001b[39;00m namedict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    712\u001b[0m     fname \u001b[38;5;241m=\u001b[39m key \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\implicit_env\\lib\\site-packages\\numpy\\lib\\npyio.py:101\u001b[0m, in \u001b[0;36mzipfile_factory\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mzipfile\u001b[39;00m\n\u001b[0;32m    100\u001b[0m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mallowZip64\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m zipfile\u001b[38;5;241m.\u001b[39mZipFile(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\implicit_env\\lib\\zipfile.py:1239\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[1;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[0;32m   1237\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m   1238\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1239\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp \u001b[38;5;241m=\u001b[39m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilemode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1240\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m   1241\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m filemode \u001b[38;5;129;01min\u001b[39;00m modeDict:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/.npz/sparse_user_item.npz'"
     ]
    }
   ],
   "source": [
    "save_npz(\"/.npz/sparse_user_item.npz\", sparse_user_item)\n",
    "save_npz(\"/.npz/sparse_item_user.npz\", sparse_item_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9a9e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.getcwd()+'/.pkl/de_als_model.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7008d2aa",
   "metadata": {},
   "source": [
    "* Criação de diferentes matrizes esparsas para operar com o algoritmo. Usuário-item e item-usuário. Cada uma deve ser usada no momento preciso\n",
    "* O alfa é o coeficiente de confiabilidade da interação do usuário com um item específico. Valor utilizado fi adotado com base no artigo: https://towardsdatascience.com/alternating-least-square-for-implicit-dataset-with-code-8e7999277f4b. Mas, podemos testar outros valores na validação do modelo.\n",
    "* Outro artigo de base pra elaboração do modelo: https://medium.com/analytics-vidhya/implementation-of-a-movies-recommender-from-implicit-feedback-6a810de173ac"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f104719b",
   "metadata": {},
   "source": [
    "# FUNÇÃO DE RECOMENDAÇÕES - IMPLICIT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46053be9",
   "metadata": {},
   "source": [
    "## Treinamento de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f300c243",
   "metadata": {},
   "outputs": [],
   "source": [
    "offers = pickle.load(open(os.getcwd()+\"/.pkl/offers.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffab5fa2",
   "metadata": {},
   "source": [
    "Carregamentodo dicionário que converte os códigos de ofertas para o seu título de oferta. Ainda falta traduzir do alemão para o inglês para tirar mais significado dos resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d08cf2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def als_model():\n",
    "    '''computes p@k and map@k evaluation metrics and saves model'''\n",
    "    \n",
    "    sparse_item_user = load_npz(os.getcwd()+\"/.npz/sparse_item_user.npz\")\n",
    "      \n",
    "    train, test = implicit.evaluation.train_test_split(sparse_item_user, train_percentage=0.8)\n",
    "\n",
    "    model = implicit.als.AlternatingLeastSquares(factors=100, \n",
    "                                                 regularization=0.1, \n",
    "                                                 iterations=20, \n",
    "                                                 calculate_training_loss=False)\n",
    "    \n",
    "    model.fit(train)\n",
    "\n",
    "    with open(model_path, 'wb') as pickle_out:\n",
    "        pickle.dump(model, pickle_out)\n",
    "    \n",
    "    return train, test, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "49bebb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluation(train, test, model): \n",
    "    \n",
    "\n",
    "    train, test = train.T.tocsr(), test.T.tocsr()\n",
    "    \n",
    "    p_at_k = implicit.evaluation.precision_at_k(model, \n",
    "                                                train_user_items=train, \n",
    "                                                test_user_items=test, \n",
    "                                                K=10, \n",
    "                                                show_progress = True)\n",
    "\n",
    "    m_at_k = implicit.evaluation.mean_average_precision_at_k(model, \n",
    "                                                             train_user_items = train, \n",
    "                                                             test_user_items = test, \n",
    "                                                             K=10, \n",
    "                                                             show_progress = True)\n",
    "\n",
    "    ndcg_at_k = implicit.evaluation.ndcg_at_k(model, \n",
    "                                              train_user_items = train,\n",
    "                                              test_user_items = test, \n",
    "                                              K=10, \n",
    "                                              show_progress = True)\n",
    "\n",
    "    auc_at_k = implicit.evaluation.AUC_at_k(model, \n",
    "                                            train_user_items = train, \n",
    "                                            test_user_items = test, \n",
    "                                            K=10, \n",
    "                                            show_progress = True)\n",
    "    metrics = {'p@K':p_at_k, \n",
    "               'map@k': m_at_k, \n",
    "               'ndcg@k':ndcg_at_k, \n",
    "               'auc@k':auc_at_k}\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895481d1",
   "metadata": {},
   "source": [
    "* Sobre metricas de precisão @k: https://medium.com/@m_n_malaeb/recall-and-precision-at-k-for-recommender-systems-618483226c54\n",
    "* Sobre NDCG: https://towardsdatascience.com/evaluate-your-recommendation-engine-using-ndcg-759a851452d1\n",
    "* Sobre Mean Average Precision: https://towardsdatascience.com/breaking-down-mean-average-precision-map-ae462f623a52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788efaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "p, m, ndcg, auc = model_evaluation(als, sparse_item_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf47972",
   "metadata": {},
   "source": [
    "Ainda não foi realizada qualquer tunagem de hiperaparâmetros. Podemos pegar alguns valores de referencia para rodar um gridsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cba99da",
   "metadata": {},
   "source": [
    "## Funções de recomendações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dca9641",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def recommend(user):\n",
    "    \n",
    "    sparse_user_item = load_npz(\"sparse_user_item.npz\")\n",
    "    \n",
    "    with open(model_path, 'rb') as pickle_in:\n",
    "        model = pickle.load(pickle_in)\n",
    "        \n",
    "    recommended, _ = zip(*model.recommend(user, sparse_user_item))\n",
    "    \n",
    "    original_user_items = list(sparse_user_item[user_id].indices)\n",
    "\n",
    "    return recommended, original_user_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "2c36b76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar_items(item_id, n_similar=10):\n",
    "    '''computes the most similar items'''\n",
    "    \n",
    "    with open(model_path, 'rb') as pickle_in:\n",
    "        model = pickle.load(pickle_in)\n",
    "\n",
    "    similar, score = zip(*model.similar_items(item_id, n_similar)[1:])\n",
    "\n",
    "    return similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "08d3d6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar_users(user_id, n_similar=10):\n",
    "    '''computes the most similar users'''\n",
    "    sparse_user_item = load_npz(os.getcwd()+\"/.npz/sparse_user_item.npz\")\n",
    "    \n",
    "    with open(model_path, 'rb') as pickle_in:\n",
    "        model = pickle.load(pickle_in)\n",
    "\n",
    "    similar, _ = zip(*model.similar_users(user_id, n_similar)[1:])\n",
    "\n",
    "    # original users items\n",
    "    original_user_items = list(sparse_user_item[user_id].indices)\n",
    "    \n",
    "    common_items_users = {}\n",
    "\n",
    "    # now we want to add the items that a similar user has rated\n",
    "    for user in similar:\n",
    "        # Verifica em cada usuário considerado similar quais são os itens que estes\n",
    "        # tem em comum com o usuário selecionado\n",
    "        common_items_users[user] = set(list(sparse_user_item[user].indices)) & set(original_user_items)\n",
    "    \n",
    "    # retorna usuários similares, e quais são os itens comuns correspondentes a cada um desses usuários\n",
    "    return similar, common_items_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7598cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recalculate_user(user_ratings):\n",
    "    '''adds new user and its liked items to sparse matrix and returns recalculated recommendations\n",
    "       Receives the user clicked products vector (user_ratings)''' \n",
    "\n",
    "    alpha = 40\n",
    "    m = load_npz('sparse_user_item.npz')\n",
    "    n_users, n_movies = m.shape\n",
    "\n",
    "    ratings = [alpha for i in range(len(user_ratings))]\n",
    "\n",
    "    m.data = np.hstack((m.data, ratings))\n",
    "    m.indices = np.hstack((m.indices, user_ratings))\n",
    "    m.indptr = np.hstack((m.indptr, len(m.data)))\n",
    "    m._shape = (n_users+1, n_movies)\n",
    "\n",
    "    # recommend N items to new user\n",
    "    with open(model_path, 'rb') as pickle_in:\n",
    "        model = pickle.load(pickle_in)\n",
    "        \n",
    "    recommended, _ =  zip(*model.recommend(n_users, m, recalculate_user=True))\n",
    "    \n",
    "    return recommended"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc30464",
   "metadata": {},
   "source": [
    "* A matriz m passa a ser a matriz com o novo usuário atualizado e é levada em consideração no para o cálculo de novos vetores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06909612",
   "metadata": {},
   "source": [
    "Nota: \n",
    "* Após os ajustes na organização das matrizes esparsas, o modelo parece não mais repetir recomendações de itens que já foram clicados pelo usuário\n",
    "* O modelo parece também não mais necessitar de tradução dos códigos de ofertas e usuário adotados na matriz esparsa para os códigos da matriz original"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d23fd9",
   "metadata": {},
   "source": [
    "### Criação e armazenamento do dicionário código-titulo de oferta (carregado na parte de cima do código)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "6171a83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = clicks_de[['Offer','OfferTitle']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "bf0517d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df_temp.drop_duplicates(['Offer','OfferTitle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "117eb228",
   "metadata": {},
   "outputs": [],
   "source": [
    "teste_dicio = dict(zip(df_temp['Offer'], df_temp['OfferTitle']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "edc5cff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(offers, open('.pkl/offers.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3024c90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
